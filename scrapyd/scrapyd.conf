[scrapyd]
# 服务监听端口
http_port=6800
# 服务监听ip, 默认是127.0.0.1
bind_address=0.0.0.0
# 启动时的scrapy进程并发数, 如果没有设置或者是0, 则是系统可用的cpu数量乘以`max_proc_per_cpu`的值, 默认是0
max_proc=0
# 每个cpu的scrapy进程并发数,默认是:4
max_proc_per_cpu=4
# 是否开启调试模式, 默认是off, 如果开启, 当调用`JSON API`发生错误时, 会返回详细的错误信息
debug=off
# scrapyd-deploy上传的egg文件的保存路径, 默认是scrapyd运行的当前目录
eggs_dir=./eggs
# scrapyd运行是的数据库路径, 包括爬虫队列信息
dbs_dir=./dbs
# Scrapy运行时的日志保存路径, 如果禁用则设置为空
logs_dir=./logs
# Scrapy运行时产生的item的数据保存路径, 因为一般都是导出或者保存到数据库, 所以这个建议禁用, 默认是禁用状态
items_dir=
# 每个爬虫运行完成后的logs和items数据保存数量, 也就是假设同一个爬虫运行到第六次, 则会删除前5次爬虫产生的数据, 这个配置项之前是`logs_to_keep`
jobs_to_keep=20
# 保存在启动器中的已完成的进程数量
finished_to_keep=40
# 轮训队列的秒数, 也就是多长时间检查一下爬虫队列
poll_interval=5
# 启动子进程的模块
# runner= scrapyd.runner
# scrapyd启动的app应用文件
# application=scrapyd.app.application
# scrapyd资源跟目录
# webroot = scrapyd.website.Root
# 节点的展示名称, 默认是${socket.gethostname()}
# node_name=

# 以下是调用接口的url路径和对应的scrapyd类, 熟悉源码可以自己进行开发
[services]
schedule.json     = scrapyd.webservice.Schedule
cancel.json       = scrapyd.webservice.Cancel
addversion.json   = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json  = scrapyd.webservice.ListSpiders
delproject.json   = scrapyd.webservice.DeleteProject
delversion.json   = scrapyd.webservice.DeleteVersion
listjobs.json     = scrapyd.webservice.ListJobs
daemonstatus.json = scrapyd.webservice.DaemonStatus